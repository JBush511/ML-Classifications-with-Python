from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss

!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv

test_df = pd.read_csv('loan_test.csv')

#PREPROCESSING the new 'loan_test.csv' File##
test_df['due_date'] = pd.to_datetime(test_df['due_date'])
test_df['effective_date'] = pd.to_datetime(test_df['effective_date'])
test_df['dayofweek'] = test_df['effective_date'].dt.dayofweek
test_df['weekend'] = test_df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)
test_df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)
test_df['Gender'].replace(to_replace=['male','female'], value=[0,1],inplace=True)
test_df.head()
test_Feature = test_df[['Principal','terms','age','Gender','weekend']]
test_df.groupby(['education'])['loan_status'].value_counts(normalize=True)
test_Feature = pd.concat([test_Feature,pd.get_dummies(test_df['education'])], axis=1)
test_Feature.drop(['Master or Above'], axis = 1,inplace=True)
test_Feature.head()

test_X = preprocessing.StandardScaler().fit(test_Feature).transform(test_Feature)
test_X[0:5]
test_y = test_df['loan_status'].values
test_y[0:5]

knn_yhat = KNeighbor.predict(test_X)
KJI = jaccard_score(test_y, knn_yhat,pos_label='PAIDOFF')
KF1 = f1_score(test_y, knn_yhat, average='weighted')
print("KNN Jaccard index: %.2f" % jaccard_score(test_y, knn_yhat,pos_label='PAIDOFF'))
print("KNN F1-score: %.2f" % f1_score(test_y, knn_yhat, average='weighted'))

#OUTPUT:
KNN Jaccard index: 0.65
KNN F1-score: 0.63

DT_yhat = DTree.predict(test_X)
DJI=jaccard_score(test_y, DT_yhat,pos_label='PAIDOFF')
DF1=f1_score(test_y, DT_yhat, average='weighted')
print("DT Jaccard index: %.2f" % jaccard_score(test_y, DT_yhat,pos_label='PAIDOFF'))
print("DT F1-score: %.2f" % f1_score(test_y, DT_yhat, average='weighted') )


#OUTPUT:
DT Jaccard index: 0.66
DT F1-score: 0.74

SVM_yhat = SVMModel.predict(test_X)
SJI=jaccard_score(test_y, SVM_yhat,pos_label='PAIDOFF')
SF1=f1_score(test_y, SVM_yhat, average='weighted')
print("SVM Jaccard index: %.2f" % jaccard_score(test_y, SVM_yhat,pos_label='PAIDOFF'))
print("SVM F1-score: %.2f" % f1_score(test_y, SVM_yhat, average='weighted') )

#OUTPUT:
SVM Jaccard index: 0.78
SVM F1-score: 0.76

LR_yhat = LR.predict(test_X)
LR_yhat_prob = LR.predict_proba(test_X)
LJI=jaccard_score(test_y, LR_yhat,pos_label='PAIDOFF')
LF1=f1_score(test_y, LR_yhat, average='weighted')
LLL=log_loss(test_y, LR_yhat_prob)
print("LR Jaccard index: %.2f" % jaccard_score(test_y, LR_yhat,pos_label='PAIDOFF'))
print("LR F1-score: %.2f" % f1_score(test_y, LR_yhat, average='weighted') )
print("LR LogLoss: %.2f" % log_loss(test_y, LR_yhat_prob))

#OUTPUT:
LR Jaccard index: 0.74
LR F1-score: 0.66
LR LogLoss: 0.57

Results = [('KNN', KJI, KF1,'Na'),('Decision Tree',DJI,DF1,'Na'),('SVM',SJI,SF1,'Na'),('Logistic Regression',LJI,LF1,LLL)]
ResultTable = pd.DataFrame(Results,columns=['Algorithm','Jaccard','F1-Score','Log Loss'])
ResultTable.head()

#OUTPUT:
	Algorithm	          Jaccard	  F1-Score	Log Loss
0	KNN	                0.653846	0.632840	Na
1	Decision Tree	      0.659091	0.736682	Na
2	SVM	                0.780000	0.758350	Na
3	Logistic Regression	0.735849	0.660427	0.567215


print(ResultTable.to_string(index=False))

#OUTPUT:
          Algorithm  Jaccard  F1-Score  Log Loss
                KNN 0.653846  0.632840        Na
      Decision Tree 0.659091  0.736682        Na
                SVM 0.780000  0.758350        Na
Logistic Regression 0.735849  0.660427  0.567215
